\documentclass[UTF8,11pt]{article}
\usepackage{fullpage}
\usepackage{CJKutf8}
\AtBeginDvi{\input{zhwinfonts}}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{indentfirst}
\textheight=8.5in
\topmargin=0in
\headheight=0in
\headsep=0in
\oddsidemargin=0in
\evensidemargin=0in
\textwidth=6in
\linespread{1.2}
\title {\vspace*{-2em}A Chinese Word Segmentation Experiment Using Conditional Random Field}
\author{Kuan-Wen Lo, Yiran Luo, Dingkang Wang, Zhaoyu Duan}
\date {\today}

\begin{document}

\numberwithin{equation}{subsection}

\maketitle

\begin{abstract}
\noindent It's well-known that the standard Chinese text has no space or other delimiter between words. Hence, word segmentation becomes the most primary task in Chinese Word Processing, and the accuracy of Chinese word segmentation is essential to the performance of following procedures, such as Parsing, Part-of-speech Tagging and Machine Translation.
\end{abstract}

\section{Introduction}
\noindent This paper is to demonstrate our study of building a Chinese word segmenter. Our system relies on a Conditional Random Filed (CRF) model, which makes binary decision labels on each character whether or not it should be segmented. Then we utilize Stochastic Gradient Descent as our training method to find the optimal parameters ($\alpha$ and $\beta$), which we train using the backward-forward algorithm to construct the probability table. Given the probability table, we eventually decode the character sequences by Viterbi's Algorithm for the most probable label sequence. Most of our algorithms are application following the tutorial by Charles Sutton and Andrew McCallum published in 2010~\cite{basicmethod}.

\section{Approaches}
\noindent In this section, we lay down the methodology of this experiment, introducing our statistic models, algorithms, and test datasets.
\subsection{Labeling the Segmentation}
\noindent Since Chinese is by nature a language with few white-spaces, we use 0 and 1 on each character to mark the non-starts and the start of each properly segmented Chinese word. For example: \newline

\begin{CJK*}{UTF8}{gbsn}
Original: 服务品质 \newline
\indent Segmented: 服务\ \ 品质 \newline
\indent Labeled sequence: (服, 1) (务, 0) (品, 1) (质, 0)
\end{CJK*}
\subsection{Conditional Random Field (CRF)}
\noindent Introduced by Lafferty et al. (2001)~\cite{crf}, Conditional Random Field is a statistical framework that helps build a discriminative model to segment and label sequential data. Later CRF was adapted for Chinese segmentation by Peng et al. (2004)~\cite{biblo0} and Chang et al. (2008)~\cite{biblo2} as a highly anticipated tool for such tasks. \newline
\indent In the big picture, we acquire the conditional log likelihood $L$ of a training set which contains N instances, for the entire source sequence set \textbf{s}, and observation sequence set \textbf{o}:
\begin{equation} \label{eq:log_likelihood}
    L = \sum_{n=1}^N \log(P( \mathbf{s}^{(n)} | \mathbf{o}^{(n)} ))
\end{equation}
\indent For each sequence pair s and o, we acquire the likelihood for all K feature functions $f_k$, along with the weights $\lambda_k$. At position $t$, feature function $f_k$ takes the arguments of the current state $s_t$, the previous state $s_{t-1}$ and the current observation $o_t$. $\lambda_k$ serves as the parameter of the CRF model.
\begin{equation} \label{eq:crf_likelihood}
    P(\mathbf{s}|\mathbf{o}) = \frac{1}{Z_{\mathbf{o}}} \exp \left( \sum_{t=1}^{T} \sum_{k=1}^K \lambda_k f_k(s_{t-1}, s_t, o_t) \right)
\end{equation}
\indent In the last setup, in addition to substituting~\ref{eq:crf_likelihood} into~\ref{eq:log_likelihood}, we have to consider overfitting, due to the fact that often the number of $\lambda$s is very large. To avoid overfitting , we need to hence place in the penalty determined by an arbitrary regularization factor $\sigma$. Eventually, our log likelihood function with penalty becomes:

\begin{equation} \label{eq:crf_log}
    L = \sum_{n=1}^{N} \frac{1}{Z_{\mathbf{o}^{(n)}}} \exp \left( \sum_{t=1}^{T} \sum_{k=1}^K \lambda_k f_k (s_{t-1}, s_t, o^{(n)}_t)\right) - \sum_{k=1}^K \frac{\lambda_k^2}{\sigma}
\end{equation}
\subsection{Stochastic Gradient Descent (SGD)}
\noindent Stochastic Gradient Descent is a simple yet very efficient approach to discriminative learning of linear classifiers under loss functions. The objective loss function often has the form of a sum:
\begin{equation}
    L(w) = \sum_{n} L_i (w)
\end{equation} 

When used to minimize the above function, a standard gradient descent method would perform the following iterations, where $\theta$ is a step size.
\begin{equation}
    w = w - \theta \nabla L (w)
\end{equation}

\subsection{Test Data}
\noindent We use Traditional Chinese corpora from Academia Sinica, Taiwan, which contains 708,953 sentences with 5,449,698 words and 8,368,050 characters~\cite{bakeoff_data} . It is the largest open-source corpora we can find and includes the most comprehensive Chinese text data. The corpora provides us segmented sentences as our training data and unsegmented sentence as our test data.

\section{Experimental Design}
\noindent This part of the paper will present how each segment of our program works based on the approaches as we have demonstrated in the previous section. 
\subsection{Preprocessing}
\noindent The training text is encoded in UTF-8 and it cannot be recognized by Python, so decoding the text first is necessary and all the spaces in the text are full-width in that we convert them to half-width space and delete newline characters ('\verb!\n!'). We tag each character with 1 or 0 which means whether the character is the beginning of the word. Then, we convert each sentence into a list of tuples which contain a character and a tag as our training data.

\subsection{Training}
\noindent According to our CRF Model, the conditional log-likelihood of a set of training instances as shown in equation~\ref{eq:crf_log}, and now, as shown in equation~\ref{eq:de_of_log}, we can take the partial derivative of the log-likelihood in order to maximize it. $F_k$ here is the frequency or count that the particular global feature $f_k$ appears in the training data.
\begin{equation} \label{eq:de_of_log}
    \frac{\partial L}{\partial \lambda_k} = \sum_{n=1}^{N} \left( F_k ( \mathbf{s}^{(n)}, \mathbf{o}^{(n)} ) - \sum_{\mathbf{s}} P( \mathbf{s} | \mathbf{o}^{(n)}) F_k ( \mathbf{s}, \mathbf{o}^{(n)} ) \right) - \frac{\lambda_k}{\sigma}
\end{equation}

After we got the expression of the partial expression, we found that the frequency term and penalty term are not hard to calculate, so we focused on how to simplify the second term $\sum_{\mathbf{s}} P( \mathbf{s} | \mathbf{o}^{(n)}) F_k ( \mathbf{s}, \mathbf{o}^{(n)} )$. If we want to compute this term directly, we need to go through every possible labeling for a given Chinese sentence, suppose that the length of the sentence is $l$, then the complexity of computing this term will be $O(l2^l)$ which is not acceptable. Therefore, we have to make it easier to computer. Equation~\ref{eq:derivative2} shows the transferred partial derivative.
\begin{equation} \label{eq:derivative2}
      \frac{\partial L}{\partial \lambda_k} = \sum_{n=1}^{N} \left( F_k ( \mathbf{s}^{(n)}, \mathbf{o}^{(n)} ) - \sum_t \sum_{s, s'}P(s_{t-1} = s, s_t = s' | \mathbf{o}^{(n)}) f_k(s, s', o^{(n)}_t) \right) - \frac{\lambda_k}{\sigma}
\end{equation}

Then, we need to define $\alpha$ table and $\beta$ table which borrow the ideas from backward-forward training. $\alpha(t, s)$ represents the probability of a partial labeling ending at position $t$ with label $s$, similarly, $\beta(t, s)$ represents the probability of a partial labeling starting at position $t+1$ with label $s$ at position $t$. Equation~\ref{eq:alpha} and equation~\ref{eq:beta} show how to compute the elements in these two table.
\begin{equation} \label{eq:alpha}
    \alpha(t, s) = \sum_{s'} \alpha(t-1, s') \exp \left( \sum_k \lambda_k f_k(s', s, o_t) \right)
\end{equation}
\begin{equation} \label{eq:beta}
    \beta(t, s) = \sum_{s'} \beta(t+1, s') \exp \left( \sum_k \lambda_k f_k(s, s', o_{t+1}) \right)
\end{equation}

At last, as shown in equation~\ref{eq:partial_prob} we can make use of $\alpha$ table, $\beta$ table and current $\lambda$ values to calculate the derivatives. $Z_{\mathbf{o}}$ here represents $P(\mathbf{o})$, and we use it here fit target probability into (0, 1), and $Z_{\mathbf{o}}$ can be calculated easily by equation~\ref{eq:zo}.
\begin{equation} \label{eq:partial_prob}
    P(s_{t-1} = s, s_{t} = s'|\mathbf{o}) = \frac{1}{Z_{\mathbf{o}}} \alpha(t-1, s) \exp \left( \sum_k \lambda_k f_k(s, s', o_t) \right) \beta(t, s')
\end{equation}
\begin{equation} \label{eq:zo}
     Z_{\mathbf{o}} = \beta(1, 0) + \beta(1, 1)
\end{equation}

Now, we are ready to implement Stochastic Gradient Descent (SGD) algorithm using the derivatives. For each parameter $\lambda_k$, after computing the derivative of the log-likelihood function, modification method of $\lambda_k$ is shown in following equation, since our goal is to find the global maximum instead of minimum, so we use addition here.
\begin{equation} \label{eq:sgd_update}
    \lambda_k = \lambda_k + \theta \frac{\partial L}{\partial \lambda_k}
\end{equation}

For the SGD, the learning rate $\theta$ can exert important influence on the final result. If learning rate $\theta$ is very small it would take so long time to change $\lambda$ to the proper values. If learning rate $\theta$ is too large, it will make the model unable to get converged. Actually, it will make the likelihood function even get a worse result.

\subsection{Decoding}
\noindent We implement Viterbi Algorithm to do decoding, which makes use of the previous states and corresponding $\lambda$ values for a given triples using equation~\ref{eq:viterbi1}. In the equation, $V(t, s)$ represent the probability of a partial labeling ending at position t with a label s, and we set $V(1, 1) = 1$ and $V(1, 0) = 0$, since the first character must be a beginning of a word.
\begin{equation} \label{eq:viterbi1}
V(t, s) = \left\{ \begin{array}{lc}
\max_{s'}( V(t-1, s') \exp \left( \sum_k \lambda_k f_k(s', s, o_t) \right) ) & t>1 \\
s==1 & t=1
\end{array} \right.
\end{equation}

In addition, in order to prevent overflow, we use the log trick to normalize the updating equation. Equation~\ref{eq:viterbi2} shows the new updating rules after normalization.
\begin{equation} \label{eq:viterbi2}
V(t, s) = \left\{ \begin{array}{lc}
\max_{s'}( V(t-1, s') + \sum_k \lambda_k f_k(s', s, o_t) ) & t>1 \\
V(1,0) = -\infty,\ V(1, 1) = 0 & t=1
\end{array} \right.
\end{equation}

\section{Experimental Results}
\noindent We have so far tested our program with three extractions out of the AS training set, each with a random selection of 20, 200, and 1000 sentences. The best performances, shown below, are scored using the 'score' script provided along the package in bakeoff2005~\cite{bakeoff_data}:
\begin{center}
\begin{tabular}{ll}
20-sentence-corpus: & \\
\multicolumn{2}{l}{~~~~TOTAL TRUE WORDS RECALL: 0.960} \\
\multicolumn{2}{l}{~~~~TOTAL TEST WORDS PRECISION: 0.954} \\
\\
200-sentence-corpus: & \\
\multicolumn{2}{l}{~~~~TOTAL TRUE WORDS RECALL: 0.774} \\
\multicolumn{2}{l}{~~~~TOTAL TEST WORDS PRECISION: 0.800} \\
\\
1000-sentence-corpus: & \\
\multicolumn{2}{l}{~~~~TOTAL TRUE WORDS RECALL: 0.465} \\
\multicolumn{2}{l}{~~~~TOTAL TEST WORDS PRECISION: 0.378} \\
\end{tabular}
\end{center}

Meanwhile, we have implemented the Stanford Chinese segmenter ported into the NLTK as a comparison model of segmentation capability, and its score based upon the 200k-sentence training data and 2000-sentence test data is:
\begin{center}
\begin{tabular}{ll}
Stanford segmenter: & \\
\multicolumn{2}{l}{~~~~TOTAL TRUE WORDS RECALL: 0.946} \\
\multicolumn{2}{l}{~~~~TOTAL TEST WORDS PRECISION: 0.951} \\
\end{tabular}
\end{center}

\section{Analysis and Discussion}
\noindent As is presented in the verdicts, our program is able to handle when the test case is relatively small. However, as the size of the test case increases, the inaccuracy floats along, leaving space for improvement in following aspects of the experiment.

\subsection{Convergence detection}
\noindent So far, we are only being able to detect manually when the parameters returned by SGD. We have only to manually control the number of loops running the regression process. Therefore, it's necessary to introduce some automatic threshold, which depends on the size of the test input, to determine when the parameter $\lambda_k$ converges.

\subsection{Unigram before bigram}
\noindent In our experiment, we only focus on bigrams, which include the previous and current states and the current observation as a whole clique. We may introduce unigram into the parameter lambda for additional features, as well as a back-off factor for unseen words in testing data.

\subsection{More precise container}
\noindent We use Python's default float type for the precision calculation, which in some cases can result in underflow as probabilities can be extremely low. We may replace float with numpy's double type for a better precision.

\section{Conclusion}
\noindent In this report, we have traced our attempt on substantiating current methods on the practice of Chinese word segmentation from scratch. First we initialize the experiment with Conditional Random Field to assign binary labels on each input sentence to decide the cuts for segmentation. Treating each sentence as a feature, we then train the model with Stochastic Gradient Descent and the backward-forward algorithm to optimize the parameters of the features. Eventually, we decode the most probable segmentation sequence using Viterbi's. 

\section{Acknowledgment}
\noindent We would like to thank Prof. Eric Fosler-Lussier for his teachings throughout the semester offering us the opportunity to furthermore explore the knowledge in NLP.

\begin{thebibliography}{99}

\bibitem{crf}John D. Lafferty, Andrew McCallum , and Fernando C. N. Pereira:
\emph{Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data} 
Proceedings of the Eighteenth International Conference on Machine Learning, p.282-289~(2001)

\bibitem{basicmethod} Charles Sutton and Andrew McCallum:
\emph{An Introduction to Conditional Random Fields}
Foundations and Trends in Machine Learning (FnT ML). ~(2010)

\bibitem{biblo0}Fuchun Peng, Fangfang Feng, and Andrew McCallum:
\emph{Chinese segmentation and new word detectionusing conditional random fields} 
In Proc. of COLING. ~(2004)

\bibitem{biblo1} Mengqiu Wang, Rob Voigt, and Christopher D.~Manning:
\emph{Two Knives Cut Better Than One: Chinese Word Segmentation with Dual Decomposition},
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL~2014): Short Papers.~(2014)

\bibitem{biblo2} Pi-Chuan Chang, Michel Galley, and Chris Manning:
\emph{Optimizing Chinese Word Segmentation for Machine Translation Performance},
In ACL Workshop on Statistical Machine Translation.~(2008)

\bibitem{biblo3} XinhaoWang, Xiaojun Lin, Dianhai Yu, Hao Tian, and XihongWu:
\emph{Chinese word segmentation with maximum entropy and n-gram language model},
In Proceedings of the fifth SIGHAN workshop on Chinese language Processing.~(2006)

\bibitem{biblo4} Zhenxing Wang, Changning Huang, and Jingbo Zhu:
\emph{The Character-based CRF Segmenter of MSRA\&NEU for the 4th Bakeoff},
Institute of Computer Software and Theory, Northeastern University,Shenyang, China, 110004, Microsoft Research Asia, Zhichun Road, Haidian District, Beijing, China, 100080.

\bibitem{biblo5} Nianwen Xue:
\emph{The Association for Computational Linguistics and Chinese Language Processing Chinese word segmentation as character tagging},
Computational Linguistics and Chinese Language Processing Vol.8, No.1, pp.29-48.~(2003)

\bibitem{bakeoff_data} Second International Chinese Word Segmentation Bakeoff Data. (n.d.). \\
\url{http://sighan.cs.uchicago.edu/bakeoff2005/}


\end{thebibliography}


\end{document} 